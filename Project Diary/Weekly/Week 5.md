# Monday

Carrying on with automating scripts - .bat files and python subprocess library to run commands and display output.
Running agents training with more hidden units and more layers (256 | 3) vs (128 | 2).
The automated start script runs the .bat file that activates the environment and opens a command prompt within it. It also runs 'mlagents-learn' command with pre-generated values.
E.g., it sets a new run ID based on the name of the previous run.
# Tuesday
I finally managed to get a GitHub repo working - the issue turned out to be extremely heavy python venv directory.
Learning about LaTeX and going through ml-agents documentation.

# Wednesday
Migrating project diary to Obsidian Vault (LaTeX).
#### Reading [ml-agents documentation](https://github.com/Unity-Technologies/ml-agents/blob/develop/docs/Background-Machine-Learning.md) on Machine Learning Background:
- **Unsupervised learning** - the goal is to group similar items in a data set. We do not provide specific examples, we just define the appropriate attributes and rely on the algorithm to uncover the clusters on its own. This data set is called unlabelled data. Quite useful when labelling is either hard or expensive to produce.
- **Supervised Learning** - not just grouping similar items but directly learn a mapping from each item to the group it belongs to. The algorithm, when provided the attributes of a new object, will output a ***predicted*** label for that object.
	#### **These two approaches require:**
	- **Attribute selection** - *feature selection* - how we wish to represent an entity / what is it represented with.
	- **Model Selection** - selecting algorithm, its parameters, to perform the task well.

- **Reinforcement Learning** - most relevant to ML-Agents Toolkit:
	- Autonomous Agents
	The goal is to learn a **policy** which maps **observations** to **actions**.
	- **Observation** - what agent can measure from its **environment**.
	- **Action** - change to the agents configuration.

	- **Reward Signal** - agent learns a policy that maximizes its overall rewards. Sparse rewards are rewards not provided at every step but when agent arrives at a success or failure situation.
	- **Learning an optimal policy** requires the agent to be placed in several action situations.
	Attribute selection defines observations for the agent to complete its objective.

**ML-Agents uses Deep Learning** to solve the attribute and selection tasks.

#### Reading [ml-agents documentation](https://github.com/Unity-Technologies/ml-agents/blob/develop/docs/Using-Tensorboard.md) on TensorBoard:
"
#### Policy Statistics
- `Policy/Entropy` (PPO; SAC) - How random the decisions of the model are.
  Should slowly decrease during a successful training process. If it decreases
  too quickly, the `beta` hyperparameter should be increased.
- `Policy/Learning Rate` (PPO; SAC) - How large a step the training algorithm
  takes as it searches for the optimal policy. Should decrease over time.
- `Policy/Entropy Coefficient` (SAC) - Determines the relative importance of the
  entropy term. This value is adjusted automatically so that the agent retains
  some amount of randomness during training.
- `Policy/Extrinsic Reward` (PPO; SAC) - This corresponds to the mean cumulative
  reward received from the environment per-episode.
- `Policy/Value Estimate` (PPO; SAC) - The mean value estimate for all states
  visited by the agent. Should increase during a successful training session.
- `Policy/Curiosity Reward` (PPO/SAC+Curiosity) - This corresponds to the mean
  cumulative intrinsic reward generated per-episode.
- `Policy/Curiosity Value Estimate` (PPO/SAC+Curiosity) - The agent's value
  estimate for the curiosity reward.
- `Policy/GAIL Reward` (PPO/SAC+GAIL) - This corresponds to the mean cumulative
  discriminator-based reward generated per-episode.
- `Policy/GAIL Value Estimate` (PPO/SAC+GAIL) - The agent's value estimate for
  the GAIL reward.
- `Policy/GAIL Policy Estimate` (PPO/SAC+GAIL) - The discriminator's estimate
  for states and actions generated by the policy.
- `Policy/GAIL Expert Estimate` (PPO/SAC+GAIL) - The discriminator's estimate
  for states and actions drawn from expert demonstrations.
#### Learning Loss Functions
- `Losses/Policy Loss` (PPO; SAC) - The mean magnitude of policy loss function.
  Correlates to how much the policy (process for deciding actions) is changing.
  The magnitude of this should decrease during a successful training session.
- `Losses/Value Loss` (PPO; SAC) - The mean loss of the value function update.
  Correlates to how well the model is able to predict the value of each state.
  This should increase while the agent is learning, and then decrease once the
  reward stabilizes.
- `Losses/Forward Loss` (PPO/SAC+Curiosity) - The mean magnitude of the forward
  model loss function. Corresponds to how well the model is able to predict the
  new observation encoding.
- `Losses/Inverse Loss` (PPO/SAC+Curiosity) - The mean magnitude of the inverse
  model loss function. Corresponds to how well the model is able to predict the
  action taken between two observations.
- `Losses/Pretraining Loss` (BC) - The mean magnitude of the behavioral cloning
  loss. Corresponds to how well the model imitates the demonstration data.
- `Losses/GAIL Loss` (GAIL) - The mean magnitude of the GAIL discriminator loss.
  Corresponds to how well the model imitates the demonstration data.
  " [Cited from](https://github.com/Unity-Technologies/ml-agents/blob/develop/docs/Using-Tensorboard.md)

# Thursday
### Daily Scrum:
#### Accomplished yesterday:
- Read through a larger portion of ML-Agents documentation.
- Started taking notes in Obsidian for better tracking and maintenance.
#### To accomplish today:
- Have a read on custom plugins for ML-Agents to capture research related data.
- Experiment with the model and environment - maybe try 3D environment (different shape).
#### Obstacles met:
- No knowledge of environment shaping in Unity
- Need to understand what the eventual plugin will have to capture.
# Friday
### Daily Scrum:
#### Accomplished yesterday:
- Read the ML-Agents documentation covering custom plugins and their implementation.
- Went through the Example code and learned about custom trainers. They seem quite complicated to achieve. though.
- Chose a few assets from the Unity Asset Store to use in the project.
- Created 2 new environments in Unity3D Terrain editor. The second one is usable for the project.
- Populated the terrain with trees and added a collider to them for more realistic results.
- Worked on the physics of the Agent - the ragdolling is quite concerning - need to be fixed with respect to the Ray Sensor.
#### To accomplish today:
- Fix the ragdolling issue - one solution is to remove rotation completely and allow the agent to adjust the angle of the Ray Sensor Component.
- Run the model in the new environments.
- Take notes of the observation.
#### Obstacles met:
- I lack knowledge of the Unity3D physics engine on that matter - some math functions would be able to achieve what I desire, however, it would take an unknown amount of time to learn them.
# Saturday
### Daily Scrum:
#### Accomplished yesterday:
- To solve the ragdoll problem, agent's rotation is locked and the additional weight is being added, for stability. The agent's collider is swapped from cube to capsule for less strict surface contact. ***NEW ISSUE CAUSED:*** Ray sensor is not rotating as well - reduces the possible scanning area for the agents.
- To solve the insufficient environment scanning, I implemented a rotation action for Ray sensor - Agent can rotate every 5 degrees up to 45 degrees each way (up and down) form the starting position. The solution tested and working successfully in a single environment run.
- Spawning in a single environment - was hard to achieve due to uneven terrain and local position in scene like-issues. Temporarily solved - to be looked into.
#### To accomplish today:
- Fix the spawning - perhaps a new algorithm
- Run the training in the new environment.
- Take notes of the observations.

#### Obstacles met:
- Spawning issue can be solved in two ways - either by creating an algorithm or by placing spawning area object (less flexible but easier and more predictable).