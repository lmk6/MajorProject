{
    "name": "root",
    "gauges": {
        "PreyController.Policy.Entropy.mean": {
            "value": 1.4308723211288452,
            "min": 1.4198116064071655,
            "max": 1.4337271451950073,
            "count": 40
        },
        "PreyController.Policy.Entropy.sum": {
            "value": 70639.3046875,
            "min": 63014.11328125,
            "max": 87254.5234375,
            "count": 40
        },
        "PreyController.Environment.EpisodeLength.mean": {
            "value": 318.03821656050957,
            "min": 299.6047904191617,
            "max": 579.8255813953489,
            "count": 40
        },
        "PreyController.Environment.EpisodeLength.sum": {
            "value": 49932.0,
            "min": 46598.0,
            "max": 53283.0,
            "count": 40
        },
        "PreyController.Step.mean": {
            "value": 1999814.0,
            "min": 49982.0,
            "max": 1999814.0,
            "count": 40
        },
        "PreyController.Step.sum": {
            "value": 1999814.0,
            "min": 49982.0,
            "max": 1999814.0,
            "count": 40
        },
        "PreyController.Policy.ExtrinsicValueEstimate.mean": {
            "value": -5.736879825592041,
            "min": -5.898193359375,
            "max": 5.639438629150391,
            "count": 40
        },
        "PreyController.Policy.ExtrinsicValueEstimate.sum": {
            "value": -900.6901245117188,
            "min": -931.91455078125,
            "max": 496.2705993652344,
            "count": 40
        },
        "PreyController.Environment.CumulativeReward.mean": {
            "value": -25.796178343949045,
            "min": -28.44311377245509,
            "max": 41.27906976744186,
            "count": 40
        },
        "PreyController.Environment.CumulativeReward.sum": {
            "value": -4050.0,
            "min": -4750.0,
            "max": 3550.0,
            "count": 40
        },
        "PreyController.Policy.ExtrinsicReward.mean": {
            "value": -25.796178343949045,
            "min": -28.44311377245509,
            "max": 41.27906976744186,
            "count": 40
        },
        "PreyController.Policy.ExtrinsicReward.sum": {
            "value": -4050.0,
            "min": -4750.0,
            "max": 3550.0,
            "count": 40
        },
        "PreyController.Losses.PolicyLoss.mean": {
            "value": 0.014518687640860057,
            "min": 0.013843614360497062,
            "max": 0.01857815749826841,
            "count": 40
        },
        "PreyController.Losses.PolicyLoss.sum": {
            "value": 0.04355606292258017,
            "min": 0.017622159241970317,
            "max": 0.05446517189440783,
            "count": 40
        },
        "PreyController.Losses.ValueLoss.mean": {
            "value": 71.60035827636719,
            "min": 31.875769271850587,
            "max": 71.60035827636719,
            "count": 40
        },
        "PreyController.Losses.ValueLoss.sum": {
            "value": 214.80107482910157,
            "min": 51.34533685048421,
            "max": 214.80107482910157,
            "count": 40
        },
        "PreyController.Policy.LearningRate.mean": {
            "value": 1.3826319508000023e-06,
            "min": 1.3826319508000023e-06,
            "max": 9.875805124195e-05,
            "count": 40
        },
        "PreyController.Policy.LearningRate.sum": {
            "value": 4.1478958524000066e-06,
            "min": 4.1478958524000066e-06,
            "max": 0.00028913086086915,
            "count": 40
        },
        "PreyController.Policy.Epsilon.mean": {
            "value": 0.10138253333333332,
            "min": 0.10138253333333332,
            "max": 0.19875805000000005,
            "count": 40
        },
        "PreyController.Policy.Epsilon.sum": {
            "value": 0.30414759999999996,
            "min": 0.19875805000000005,
            "max": 0.58913085,
            "count": 40
        },
        "PreyController.Policy.Beta.mean": {
            "value": 7.898841333333344e-05,
            "min": 7.898841333333344e-05,
            "max": 0.004938026694999999,
            "count": 40
        },
        "PreyController.Policy.Beta.sum": {
            "value": 0.00023696524000000035,
            "min": 0.00023696524000000035,
            "max": 0.014457629415,
            "count": 40
        },
        "PreyController.IsTraining.mean": {
            "value": 1.0,
            "min": 1.0,
            "max": 1.0,
            "count": 40
        },
        "PreyController.IsTraining.sum": {
            "value": 1.0,
            "min": 1.0,
            "max": 1.0,
            "count": 40
        },
        "HunterController.Policy.Entropy.mean": {
            "value": 1.4076319932937622,
            "min": 1.4076319932937622,
            "max": 1.4191733598709106,
            "count": 40
        },
        "HunterController.Policy.Entropy.sum": {
            "value": 69491.9765625,
            "min": 55332.6875,
            "max": 91688.171875,
            "count": 40
        },
        "HunterController.Environment.EpisodeLength.mean": {
            "value": 318.03821656050957,
            "min": 299.6047904191617,
            "max": 583.1780821917808,
            "count": 40
        },
        "HunterController.Environment.EpisodeLength.sum": {
            "value": 49932.0,
            "min": 42146.0,
            "max": 58524.0,
            "count": 40
        },
        "HunterController.Step.mean": {
            "value": 1999814.0,
            "min": 49982.0,
            "max": 1999814.0,
            "count": 40
        },
        "HunterController.Step.sum": {
            "value": 1999814.0,
            "min": 49982.0,
            "max": 1999814.0,
            "count": 40
        },
        "HunterController.Policy.ExtrinsicValueEstimate.mean": {
            "value": -2.55580472946167,
            "min": -17.56601905822754,
            "max": -0.6902215480804443,
            "count": 40
        },
        "HunterController.Policy.ExtrinsicValueEstimate.sum": {
            "value": -401.2613525390625,
            "min": -1557.076171875,
            "max": -71.78304290771484,
            "count": 40
        },
        "HunterController.Environment.CumulativeReward.mean": {
            "value": -11.268789443240804,
            "min": -119.01279099043026,
            "max": -1.7152694941994673,
            "count": 40
        },
        "HunterController.Environment.CumulativeReward.sum": {
            "value": -1769.1999425888062,
            "min": -11750.79988861084,
            "max": -286.45000553131104,
            "count": 40
        },
        "HunterController.Policy.ExtrinsicReward.mean": {
            "value": -11.268789443240804,
            "min": -119.01279099043026,
            "max": -1.7152694941994673,
            "count": 40
        },
        "HunterController.Policy.ExtrinsicReward.sum": {
            "value": -1769.1999425888062,
            "min": -11750.79988861084,
            "max": -286.45000553131104,
            "count": 40
        },
        "HunterController.Losses.PolicyLoss.mean": {
            "value": 0.012587750388047426,
            "min": 0.009570262564811855,
            "max": 0.014641414717771114,
            "count": 40
        },
        "HunterController.Losses.PolicyLoss.sum": {
            "value": 0.012587750388047426,
            "min": 0.009570262564811855,
            "max": 0.02673345759510994,
            "count": 40
        },
        "HunterController.Losses.ValueLoss.mean": {
            "value": 55.57707984924316,
            "min": 10.49965036392212,
            "max": 82.48857475280762,
            "count": 40
        },
        "HunterController.Losses.ValueLoss.sum": {
            "value": 55.57707984924316,
            "min": 10.49965036392212,
            "max": 114.95945312500001,
            "count": 40
        },
        "HunterController.Policy.LearningRate.mean": {
            "value": 8.8404911605e-07,
            "min": 8.8404911605e-07,
            "max": 9.793500206499999e-05,
            "count": 40
        },
        "HunterController.Policy.LearningRate.sum": {
            "value": 8.8404911605e-07,
            "min": 8.8404911605e-07,
            "max": 0.0001773201226799,
            "count": 40
        },
        "HunterController.Policy.Epsilon.mean": {
            "value": 0.10088395000000001,
            "min": 0.10088395000000001,
            "max": 0.19793499999999997,
            "count": 40
        },
        "HunterController.Policy.Epsilon.sum": {
            "value": 0.10088395000000001,
            "min": 0.10088395000000001,
            "max": 0.37732010000000005,
            "count": 40
        },
        "HunterController.Policy.Beta.mean": {
            "value": 5.4109105000000014e-05,
            "min": 5.4109105000000014e-05,
            "max": 0.0048969565,
            "count": 40
        },
        "HunterController.Policy.Beta.sum": {
            "value": 5.4109105000000014e-05,
            "min": 5.4109105000000014e-05,
            "max": 0.00886827299,
            "count": 40
        },
        "HunterController.IsTraining.mean": {
            "value": 1.0,
            "min": 1.0,
            "max": 1.0,
            "count": 40
        },
        "HunterController.IsTraining.sum": {
            "value": 1.0,
            "min": 1.0,
            "max": 1.0,
            "count": 40
        }
    },
    "metadata": {
        "timer_format_version": "0.1.0",
        "start_time_seconds": "1713514137",
        "python_version": "3.10.12 | packaged by Anaconda, Inc. | (main, Jul  5 2023, 19:01:18) [MSC v.1916 64 bit (AMD64)]",
        "command_line_arguments": "C:\\Users\\Lukas\\miniconda3\\envs\\mlagents\\Scripts\\mlagents-learn C:\\MajorProject\\MP_ML_agents\\Assets\\TrainConfigMAgents.yaml --run-id=PreyVsHunter_51",
        "mlagents_version": "1.0.0",
        "mlagents_envs_version": "1.0.0",
        "communication_protocol_version": "1.5.0",
        "pytorch_version": "2.2.0+cpu",
        "numpy_version": "1.23.1",
        "end_time_seconds": "1713517361"
    },
    "total": 3223.6037325999996,
    "count": 1,
    "self": 0.009281499999815424,
    "children": {
        "run_training.setup": {
            "total": 0.06211689999986447,
            "count": 1,
            "self": 0.06211689999986447
        },
        "TrainerController.start_learning": {
            "total": 3223.5323341999997,
            "count": 1,
            "self": 0.8779851999893253,
            "children": {
                "TrainerController._reset_env": {
                    "total": 3.8014690999998493,
                    "count": 1,
                    "self": 3.8014690999998493
                },
                "TrainerController.advance": {
                    "total": 3218.7098781000104,
                    "count": 42992,
                    "self": 0.3905977000031271,
                    "children": {
                        "env_step": {
                            "total": 3218.3192804000073,
                            "count": 42992,
                            "self": 2658.048671299949,
                            "children": {
                                "SubprocessEnvManager._take_step": {
                                    "total": 559.8394686000106,
                                    "count": 42992,
                                    "self": 5.653857300016625,
                                    "children": {
                                        "TorchPolicy.evaluate": {
                                            "total": 554.185611299994,
                                            "count": 78886,
                                            "self": 554.185611299994
                                        }
                                    }
                                },
                                "workers": {
                                    "total": 0.43114050004783167,
                                    "count": 42992,
                                    "self": 0.0,
                                    "children": {
                                        "worker_root": {
                                            "total": 3219.5255397999977,
                                            "count": 42992,
                                            "is_parallel": true,
                                            "self": 1071.9973453000084,
                                            "children": {
                                                "steps_from_proto": {
                                                    "total": 0.002560899999934918,
                                                    "count": 2,
                                                    "is_parallel": true,
                                                    "self": 0.00016489999961777357,
                                                    "children": {
                                                        "_process_rank_one_or_two_observation": {
                                                            "total": 0.0023960000003171444,
                                                            "count": 8,
                                                            "is_parallel": true,
                                                            "self": 0.0023960000003171444
                                                        }
                                                    }
                                                },
                                                "UnityEnvironment.step": {
                                                    "total": 2147.5256335999893,
                                                    "count": 42992,
                                                    "is_parallel": true,
                                                    "self": 28.41023840003436,
                                                    "children": {
                                                        "UnityEnvironment._generate_step_input": {
                                                            "total": 30.076350500003628,
                                                            "count": 42992,
                                                            "is_parallel": true,
                                                            "self": 30.076350500003628
                                                        },
                                                        "communicator.exchange": {
                                                            "total": 2012.2716572999914,
                                                            "count": 42992,
                                                            "is_parallel": true,
                                                            "self": 2012.2716572999914
                                                        },
                                                        "steps_from_proto": {
                                                            "total": 76.76738739995972,
                                                            "count": 85984,
                                                            "is_parallel": true,
                                                            "self": 7.36176279988149,
                                                            "children": {
                                                                "_process_rank_one_or_two_observation": {
                                                                    "total": 69.40562460007823,
                                                                    "count": 343936,
                                                                    "is_parallel": true,
                                                                    "self": 69.40562460007823
                                                                }
                                                            }
                                                        }
                                                    }
                                                }
                                            }
                                        }
                                    }
                                }
                            }
                        }
                    }
                },
                "trainer_threads": {
                    "total": 3.6299999919719994e-05,
                    "count": 1,
                    "self": 3.6299999919719994e-05,
                    "children": {
                        "thread_root": {
                            "total": 0.0,
                            "count": 0,
                            "is_parallel": true,
                            "self": 0.0,
                            "children": {
                                "trainer_advance": {
                                    "total": 6436.830048699997,
                                    "count": 262216,
                                    "is_parallel": true,
                                    "self": 8.166753599926778,
                                    "children": {
                                        "process_trajectory": {
                                            "total": 4580.505680000068,
                                            "count": 262216,
                                            "is_parallel": true,
                                            "self": 4579.873623100068,
                                            "children": {
                                                "RLTrainer._checkpoint": {
                                                    "total": 0.632056900000407,
                                                    "count": 8,
                                                    "is_parallel": true,
                                                    "self": 0.632056900000407
                                                }
                                            }
                                        },
                                        "_update_policy": {
                                            "total": 1848.1576151000017,
                                            "count": 143,
                                            "is_parallel": true,
                                            "self": 624.7931787999914,
                                            "children": {
                                                "TorchPPOOptimizer.update": {
                                                    "total": 1223.3644363000103,
                                                    "count": 7185,
                                                    "is_parallel": true,
                                                    "self": 1223.3644363000103
                                                }
                                            }
                                        }
                                    }
                                }
                            }
                        }
                    }
                },
                "TrainerController._save_models": {
                    "total": 0.14296550000017305,
                    "count": 1,
                    "self": 0.04503260000001319,
                    "children": {
                        "RLTrainer._checkpoint": {
                            "total": 0.09793290000015986,
                            "count": 2,
                            "self": 0.09793290000015986
                        }
                    }
                }
            }
        }
    }
}