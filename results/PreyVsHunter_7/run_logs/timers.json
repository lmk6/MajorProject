{
    "name": "root",
    "gauges": {
        "HunterController.Policy.Entropy.mean": {
            "value": 1.371655821800232,
            "min": 1.371655821800232,
            "max": 1.4160089492797852,
            "count": 15
        },
        "HunterController.Policy.Entropy.sum": {
            "value": 68765.21875,
            "min": 68584.203125,
            "max": 72360.890625,
            "count": 15
        },
        "HunterController.Environment.EpisodeLength.mean": {
            "value": 162.77302631578948,
            "min": 162.77302631578948,
            "max": 408.4375,
            "count": 15
        },
        "HunterController.Environment.EpisodeLength.sum": {
            "value": 49483.0,
            "min": 44558.0,
            "max": 52274.0,
            "count": 15
        },
        "HunterController.Step.mean": {
            "value": 749963.0,
            "min": 49949.0,
            "max": 749963.0,
            "count": 15
        },
        "HunterController.Step.sum": {
            "value": 749963.0,
            "min": 49949.0,
            "max": 749963.0,
            "count": 15
        },
        "HunterController.Policy.ExtrinsicValueEstimate.mean": {
            "value": -170.46490478515625,
            "min": -185.00418090820312,
            "max": -41.290985107421875,
            "count": 15
        },
        "HunterController.Policy.ExtrinsicValueEstimate.sum": {
            "value": -160237.015625,
            "min": -160237.015625,
            "max": -33404.40625,
            "count": 15
        },
        "HunterController.Environment.CumulativeReward.mean": {
            "value": -453.28366256310994,
            "min": -1005.9560769161332,
            "max": -453.28366256310994,
            "count": 15
        },
        "HunterController.Environment.CumulativeReward.sum": {
            "value": -137344.9497566223,
            "min": -137656.94952011108,
            "max": -107637.30023002625,
            "count": 15
        },
        "HunterController.Policy.ExtrinsicReward.mean": {
            "value": -453.28366256310994,
            "min": -1005.9560769161332,
            "max": -453.28366256310994,
            "count": 15
        },
        "HunterController.Policy.ExtrinsicReward.sum": {
            "value": -137344.9497566223,
            "min": -137656.94952011108,
            "max": -107637.30023002625,
            "count": 15
        },
        "HunterController.Losses.PolicyLoss.mean": {
            "value": 0.02295477771394265,
            "min": 0.02081150944218583,
            "max": 0.027676534252355906,
            "count": 15
        },
        "HunterController.Losses.PolicyLoss.sum": {
            "value": 0.11477388856971325,
            "min": 0.0921803356798288,
            "max": 0.12862301387746508,
            "count": 15
        },
        "HunterController.Losses.ValueLoss.mean": {
            "value": 310.0062104288737,
            "min": 310.0062104288737,
            "max": 428.78215759277344,
            "count": 15
        },
        "HunterController.Losses.ValueLoss.sum": {
            "value": 1550.0310521443685,
            "min": 1402.7735773722331,
            "max": 2143.9107879638673,
            "count": 15
        },
        "HunterController.Policy.LearningRate.mean": {
            "value": 1.0436816521093338e-05,
            "min": 1.0436816521093338e-05,
            "max": 0.0002887786037404667,
            "count": 15
        },
        "HunterController.Policy.LearningRate.sum": {
            "value": 5.218408260546669e-05,
            "min": 5.218408260546669e-05,
            "max": 0.001349353650215467,
            "count": 15
        },
        "HunterController.Policy.Epsilon.mean": {
            "value": 0.10347890666666668,
            "min": 0.10347890666666668,
            "max": 0.19625953333333335,
            "count": 15
        },
        "HunterController.Policy.Epsilon.sum": {
            "value": 0.5173945333333334,
            "min": 0.49355453333333343,
            "max": 0.9497845333333335,
            "count": 15
        },
        "HunterController.Policy.Beta.mean": {
            "value": 0.00018359744266666675,
            "min": 0.00018359744266666675,
            "max": 0.004813350713333333,
            "count": 15
        },
        "HunterController.Policy.Beta.sum": {
            "value": 0.0009179872133333337,
            "min": 0.0009179872133333337,
            "max": 0.02249424821333333,
            "count": 15
        },
        "HunterController.IsTraining.mean": {
            "value": 1.0,
            "min": 1.0,
            "max": 1.0,
            "count": 15
        },
        "HunterController.IsTraining.sum": {
            "value": 1.0,
            "min": 1.0,
            "max": 1.0,
            "count": 15
        },
        "PreyController.Policy.Entropy.mean": {
            "value": 1.427744746208191,
            "min": 1.4218546152114868,
            "max": 1.4358984231948853,
            "count": 15
        },
        "PreyController.Policy.Entropy.sum": {
            "value": 71577.125,
            "min": 71243.5390625,
            "max": 72587.1015625,
            "count": 15
        },
        "PreyController.Environment.EpisodeLength.mean": {
            "value": 162.77302631578948,
            "min": 162.77302631578948,
            "max": 408.4375,
            "count": 15
        },
        "PreyController.Environment.EpisodeLength.sum": {
            "value": 49483.0,
            "min": 44558.0,
            "max": 52274.0,
            "count": 15
        },
        "PreyController.Step.mean": {
            "value": 749963.0,
            "min": 49949.0,
            "max": 749963.0,
            "count": 15
        },
        "PreyController.Step.sum": {
            "value": 749963.0,
            "min": 49949.0,
            "max": 749963.0,
            "count": 15
        },
        "PreyController.Policy.ExtrinsicValueEstimate.mean": {
            "value": 1.3035533428192139,
            "min": 0.5443322658538818,
            "max": 4.707658290863037,
            "count": 15
        },
        "PreyController.Policy.ExtrinsicValueEstimate.sum": {
            "value": 1225.340087890625,
            "min": 440.36480712890625,
            "max": 3864.987548828125,
            "count": 15
        },
        "PreyController.Environment.CumulativeReward.mean": {
            "value": 0.9900990099009901,
            "min": -1.2096774193548387,
            "max": 20.967741935483872,
            "count": 15
        },
        "PreyController.Environment.CumulativeReward.sum": {
            "value": 300.0,
            "min": -300.0,
            "max": 2600.0,
            "count": 15
        },
        "PreyController.Policy.ExtrinsicReward.mean": {
            "value": 0.9900990099009901,
            "min": -1.2096774193548387,
            "max": 20.967741935483872,
            "count": 15
        },
        "PreyController.Policy.ExtrinsicReward.sum": {
            "value": 300.0,
            "min": -300.0,
            "max": 2600.0,
            "count": 15
        },
        "PreyController.Losses.PolicyLoss.mean": {
            "value": 0.022630147101687422,
            "min": 0.020829657194456862,
            "max": 0.025782835985980152,
            "count": 15
        },
        "PreyController.Losses.PolicyLoss.sum": {
            "value": 0.11315073550843711,
            "min": 0.0849305685182723,
            "max": 0.12891417992990076,
            "count": 15
        },
        "PreyController.Losses.ValueLoss.mean": {
            "value": 26.270675296783452,
            "min": 26.270675296783452,
            "max": 70.15658475240072,
            "count": 15
        },
        "PreyController.Losses.ValueLoss.sum": {
            "value": 131.35337648391726,
            "min": 118.82389262517295,
            "max": 350.7829237620036,
            "count": 15
        },
        "PreyController.Policy.LearningRate.mean": {
            "value": 1.0436816521093338e-05,
            "min": 1.0436816521093338e-05,
            "max": 0.0002887786037404667,
            "count": 15
        },
        "PreyController.Policy.LearningRate.sum": {
            "value": 5.218408260546669e-05,
            "min": 5.218408260546669e-05,
            "max": 0.001349353650215467,
            "count": 15
        },
        "PreyController.Policy.Epsilon.mean": {
            "value": 0.10347890666666668,
            "min": 0.10347890666666668,
            "max": 0.19625953333333335,
            "count": 15
        },
        "PreyController.Policy.Epsilon.sum": {
            "value": 0.5173945333333334,
            "min": 0.49355453333333343,
            "max": 0.9497845333333335,
            "count": 15
        },
        "PreyController.Policy.Beta.mean": {
            "value": 0.00018359744266666675,
            "min": 0.00018359744266666675,
            "max": 0.004813350713333333,
            "count": 15
        },
        "PreyController.Policy.Beta.sum": {
            "value": 0.0009179872133333337,
            "min": 0.0009179872133333337,
            "max": 0.02249424821333333,
            "count": 15
        },
        "PreyController.IsTraining.mean": {
            "value": 1.0,
            "min": 1.0,
            "max": 1.0,
            "count": 15
        },
        "PreyController.IsTraining.sum": {
            "value": 1.0,
            "min": 1.0,
            "max": 1.0,
            "count": 15
        }
    },
    "metadata": {
        "timer_format_version": "0.1.0",
        "start_time_seconds": "1709817179",
        "python_version": "3.10.12 | packaged by Anaconda, Inc. | (main, Jul  5 2023, 19:01:18) [MSC v.1916 64 bit (AMD64)]",
        "command_line_arguments": "C:\\Users\\Lukas\\miniconda3\\envs\\mlagents\\Scripts\\mlagents-learn C:\\MajorProject\\MP_ML_agents\\Assets\\TrainConfigMAgents.yaml --run-id=PreyVsHunter_7",
        "mlagents_version": "1.0.0",
        "mlagents_envs_version": "1.0.0",
        "communication_protocol_version": "1.5.0",
        "pytorch_version": "2.2.0+cpu",
        "numpy_version": "1.23.1",
        "end_time_seconds": "1709817986"
    },
    "total": 806.6328713000003,
    "count": 1,
    "self": 0.011767100000724895,
    "children": {
        "run_training.setup": {
            "total": 0.059249699999782024,
            "count": 1,
            "self": 0.059249699999782024
        },
        "TrainerController.start_learning": {
            "total": 806.5618544999998,
            "count": 1,
            "self": 0.26402650007003103,
            "children": {
                "TrainerController._reset_env": {
                    "total": 7.496355400000539,
                    "count": 1,
                    "self": 7.496355400000539
                },
                "TrainerController.advance": {
                    "total": 798.7133010999296,
                    "count": 16996,
                    "self": 0.131131799902505,
                    "children": {
                        "env_step": {
                            "total": 798.5821693000271,
                            "count": 16996,
                            "self": 681.7681245002132,
                            "children": {
                                "SubprocessEnvManager._take_step": {
                                    "total": 116.67954179996923,
                                    "count": 16996,
                                    "self": 1.6082726000822731,
                                    "children": {
                                        "TorchPolicy.evaluate": {
                                            "total": 115.07126919988696,
                                            "count": 29482,
                                            "self": 115.07126919988696
                                        }
                                    }
                                },
                                "workers": {
                                    "total": 0.13450299984469893,
                                    "count": 16996,
                                    "self": 0.0,
                                    "children": {
                                        "worker_root": {
                                            "total": 799.6875682998216,
                                            "count": 16996,
                                            "is_parallel": true,
                                            "self": 279.3930391998347,
                                            "children": {
                                                "steps_from_proto": {
                                                    "total": 0.0017358000004605856,
                                                    "count": 2,
                                                    "is_parallel": true,
                                                    "self": 0.00017440000010537915,
                                                    "children": {
                                                        "_process_rank_one_or_two_observation": {
                                                            "total": 0.0015614000003552064,
                                                            "count": 8,
                                                            "is_parallel": true,
                                                            "self": 0.0015614000003552064
                                                        }
                                                    }
                                                },
                                                "UnityEnvironment.step": {
                                                    "total": 520.2927932999864,
                                                    "count": 16996,
                                                    "is_parallel": true,
                                                    "self": 9.423769700109915,
                                                    "children": {
                                                        "UnityEnvironment._generate_step_input": {
                                                            "total": 13.506306400046014,
                                                            "count": 16996,
                                                            "is_parallel": true,
                                                            "self": 13.506306400046014
                                                        },
                                                        "communicator.exchange": {
                                                            "total": 473.5852533000052,
                                                            "count": 16996,
                                                            "is_parallel": true,
                                                            "self": 473.5852533000052
                                                        },
                                                        "steps_from_proto": {
                                                            "total": 23.777463899825307,
                                                            "count": 33992,
                                                            "is_parallel": true,
                                                            "self": 3.0803492999475566,
                                                            "children": {
                                                                "_process_rank_one_or_two_observation": {
                                                                    "total": 20.69711459987775,
                                                                    "count": 135968,
                                                                    "is_parallel": true,
                                                                    "self": 20.69711459987775
                                                                }
                                                            }
                                                        }
                                                    }
                                                }
                                            }
                                        }
                                    }
                                }
                            }
                        }
                    }
                },
                "trainer_threads": {
                    "total": 2.6699999580159783e-05,
                    "count": 1,
                    "self": 2.6699999580159783e-05,
                    "children": {
                        "thread_root": {
                            "total": 0.0,
                            "count": 0,
                            "is_parallel": true,
                            "self": 0.0,
                            "children": {
                                "trainer_advance": {
                                    "total": 1597.3598281002905,
                                    "count": 77758,
                                    "is_parallel": true,
                                    "self": 1.8814838001653698,
                                    "children": {
                                        "process_trajectory": {
                                            "total": 1101.0241643001355,
                                            "count": 77758,
                                            "is_parallel": true,
                                            "self": 1100.6782136001348,
                                            "children": {
                                                "RLTrainer._checkpoint": {
                                                    "total": 0.3459507000006852,
                                                    "count": 2,
                                                    "is_parallel": true,
                                                    "self": 0.3459507000006852
                                                }
                                            }
                                        },
                                        "_update_policy": {
                                            "total": 494.4541799999897,
                                            "count": 144,
                                            "is_parallel": true,
                                            "self": 178.30932900001244,
                                            "children": {
                                                "TorchPPOOptimizer.update": {
                                                    "total": 316.1448509999773,
                                                    "count": 4332,
                                                    "is_parallel": true,
                                                    "self": 316.1448509999773
                                                }
                                            }
                                        }
                                    }
                                }
                            }
                        }
                    }
                },
                "TrainerController._save_models": {
                    "total": 0.08814480000000913,
                    "count": 1,
                    "self": 0.027344199999788543,
                    "children": {
                        "RLTrainer._checkpoint": {
                            "total": 0.060800600000220584,
                            "count": 2,
                            "self": 0.060800600000220584
                        }
                    }
                }
            }
        }
    }
}